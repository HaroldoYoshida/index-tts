# Agents Instructions - IndexTTS2

> **√öltima Atualiza√ß√£o:** 2026-01-08
> **Projeto:** IndexTTS2 - Text-to-Speech para **Anime Studio Auto**

---

## üéØ OBJETIVO DO PROJETO

IndexTTS2 √© o motor TTS para produ√ß√£o de animes no **Anime-Studio-Auto**:
- **Voice Cloning** - Clonagem de voz zero-shot (vozes de personagens)
- **Emotion Control** - Controle emocional para cenas dram√°ticas
- **Duration Control** - Sincroniza√ß√£o precisa com lip-sync
- **Multilingual** - Portugu√™s BR + Japon√™s para anime

### Integra√ß√£o com Anime Studio
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ANIME-STUDIO-AUTO PIPELINE (MULTI-GPU)                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  N√ì: gpu-node-3090 (Dual 3090)                                  ‚îÇ
‚îÇ  ‚îú‚îÄ GPU 0: [Inspector] Cosmos Reason (Vision)                   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ Analisa refer√™ncia, extrai beats visuais                 ‚îÇ
‚îÇ  ‚îî‚îÄ GPU 1: [Director] LLM Local (Qwen 14B/32B)                  ‚îÇ
‚îÇ     ‚îî‚îÄ Analisa roteiro, define emo√ß√µes p/ TTS                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  N√ì: gpu-node-4090 (Single 4090)                                ‚îÇ
‚îÇ  ‚îú‚îÄ [Producer] IndexTTS2 + Wan 2.1 T2V                          ‚îÇ
‚îÇ  ‚îî‚îÄ Gera √Åudio Final (com emo√ß√£o) e V√≠deo Final                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üñ•Ô∏è INFRAESTRUTURA

### Servidores Dispon√≠veis (IPs Tailscale)

| Servidor             | Tailscale IP     | Hardware            | Uso no Pipeline                           |
| -------------------- | ---------------- | ------------------- | ----------------------------------------- |
| **dev-remote-01**    | `10.41.80.17`    | VM Linux            | **Desenvolvimento Principal** (IndexTTS2) |
| **gpu-node-4090**    | `100.114.21.15`  | RTX 4090 24GB       | IndexTTS2 + Wan T2V (Producer)            |
| **gpu-node-3090**    | `100.86.51.120`  | 2x RTX 3090 48GB    | Cosmos Reason + ComfyUI (Inspector)       |
| **data-services-01** | `100.103.114.73` | VM Linux            | PostgreSQL, Redis, Control Plane          |
| **gpac-teste-01**    | `100.67.24.51`   | 32 cores, 787GB RAM | LLM Qwen3-235B (Roteiros)                 |
| **minio-01~04**      | `100.109.125.15` | Cluster 4 nodes     | MinIO Object Storage (Assets)             |
| **macmini-m4-01**    | `100.72.90.72`   | M4 16GB             | MLX LLM API, Host WoL                     |
| **windows-wsl**      | `100.66.250.109` | RTX 5060 Ti Win     | Fonte de Dados (AnimeWwise)               |

### PC Windows (Fonte de Dados)

**Host:** `DESKTOP-5FKDHQF` | **IP:** `100.66.250.109` | **Alias SSH:** `windows-local`

```bash
# Acesso SSH por Certificado (sem senha!)
ssh windows-local

# Ou explicitamente:
ssh user@100.66.250.109

# Localiza√ß√£o do AnimeWwise (12TB drive E:)
# E:\AnimeWwise\Genshin_JP\vo_freetalk\vo_*
# E:\AnimeWwise\ZZZ_JP\*
```

**Datasets Dispon√≠veis:**
| Dataset    | Personagens | Caminho Windows                         |
| ---------- | ----------- | --------------------------------------- |
| Genshin_JP | 110+ chars  | `E:\AnimeWwise\Genshin_JP\vo_freetalk\` |
| Genshin_EN | ~100 chars  | `E:\AnimeWwise\Genshin_EN\`             |
| ZZZ_JP     | Ver 1.4-2.2 | `E:\AnimeWwise\ZZZ_JP\`                 |
| ZZZ_EN     | Ver 1.4-2.2 | `E:\AnimeWwise\ZZZ_EN\`                 |

### GPU Principal: `gpu-node-4090` (100.114.21.15)

**Status Atual:** ‚úÖ Online - 24GB VRAM livres

**Hardware:**
- NVIDIA RTX 4090 (24GB VRAM)
- Ubuntu 24.04
- Docker + NVIDIA Container Toolkit
- CUDA 12.8 / Driver 580.95

**Workloads na 4090:**
1. **IndexTTS2** - TTS (~8GB FP16)
2. **Wan 2.1 T2V** - Gera√ß√£o de v√≠deo (~16GB)
3. **Treino LoRA** - musubi-tuner

**Requisitos IndexTTS2:**
- ~8GB VRAM (FP16) / ~14GB VRAM (FP32)
- PyTorch 2.8 + CUDA 12.8

---

## üîß ACESSO SSH

```bash
# GPU Node 4090 (principal para IndexTTS2)
ssh vmadmin@100.114.21.15

# GPU Node 3090 (backup)
ssh vmadmin@100.86.51.120

# Data Services
ssh vmadmin@100.103.114.73

# LLM Server (CPU)
ssh vmadmin@100.67.24.51
```

---

## ‚ö° COMANDOS R√ÅPIDOS IndexTTS2

### Verificar GPU
```bash
# Verificar CUDA dispon√≠vel
uv run tools/gpu_check.py

# Monitorar VRAM
nvidia-smi -l 1
```

### Rodar WebUI
```bash
# WebUI padr√£o (FP32)
uv run webui.py

# WebUI com FP16 (menor VRAM, recomendado)
uv run webui.py --fp16

# Ver todas as op√ß√µes
uv run webui.py -h
```

### Infer√™ncia via Python
```python
from indextts.infer_v2 import IndexTTS2

tts = IndexTTS2(
    cfg_path="checkpoints/config.yaml", 
    model_dir="checkpoints", 
    use_fp16=True,  # Usar FP16 para menor VRAM
    use_cuda_kernel=False, 
    use_deepspeed=False
)

# Voice cloning simples
tts.infer(
    spk_audio_prompt='examples/voice_01.wav', 
    text="Ol√°, este √© um teste!", 
    output_path="output.wav", 
    verbose=True
)

# Com controle emocional via √°udio
tts.infer(
    spk_audio_prompt='examples/voice_07.wav', 
    text="Este texto ser√° falado com emo√ß√£o triste.",
    emo_audio_prompt='examples/emo_sad.wav',
    emo_alpha=0.9,
    output_path="output_sad.wav"
)

# Com controle emocional via vetor
# [happy, angry, sad, afraid, disgusted, melancholic, surprised, calm]
tts.infer(
    spk_audio_prompt='examples/voice_10.wav', 
    text="Que surpresa incr√≠vel!",
    emo_vector=[0, 0, 0, 0, 0, 0, 0.8, 0],  # Surprised
    output_path="output_surprised.wav"
)

# Com emo√ß√£o baseada no texto
tts.infer(
    spk_audio_prompt='examples/voice_12.wav', 
    text="Corra! Eles est√£o vindo!",
    use_emo_text=True,
    emo_alpha=0.6,
    output_path="output_afraid.wav"
)
```

---

## üéØ MELHORES PR√ÅTICAS (Pesquisa MCP)

> Fonte: Perplexity + Context7 (IndexTTS, index-tts-lora)

### √Åudio de Refer√™ncia (Voice Cloning)
| Par√¢metro       | Recomenda√ß√£o                 |
| --------------- | ---------------------------- |
| **Dura√ß√£o**     | 3-10 segundos (m√°x ~1:30)    |
| **Sample Rate** | 16kHz+ (24kHz ideal)         |
| **Qualidade**   | √Åudio limpo, sem ru√≠do/eco   |
| **Conte√∫do**    | Fala cont√≠nua, emo√ß√£o neutra |

‚ö†Ô∏è **Textos longos**: Segmentar para evitar degrada√ß√£o de qualidade.

### Controle Emocional - 3 Modos

**1. Via √Åudio de Refer√™ncia (mais natural):**
```python
tts.infer(
    spk_audio_prompt='voz_personagem.wav',  # Timbre
    emo_audio_prompt='emo√ß√£o_triste.wav',    # Emo√ß√£o (outro speaker OK!)
    emo_alpha=0.9,  # 0.0-1.0 intensidade
    text="Texto com emo√ß√£o aplicada"
)
```

**2. Via Vetor de Emo√ß√µes (mais preciso):**
```python
# [happy, angry, sad, afraid, disgusted, melancholic, surprised, calm]
emo_vector = [0.3, 0, 0.5, 0, 0, 0.2, 0, 0]  # Mix: happy + sad + melancholic

tts.infer(
    spk_audio_prompt='voz.wav',
    emo_vector=emo_vector,
    use_random=False,  # True reduz fidelidade da clonagem
    text="Texto"
)
```

**3. Via Texto Natural (mais f√°cil):**
```python
tts.infer(
    spk_audio_prompt='voz.wav',
    use_emo_text=True,       # Detecta emo√ß√£o do texto automaticamente
    emo_text="Estou com medo!",  # Ou define explicitamente
    emo_alpha=0.6,           # Recomendado ~0.6 para modo texto
    text="Corra! Eles est√£o vindo!"
)
```

### Otimiza√ß√£o de Infer√™ncia

| Flag                   | Efeito                  | Recomenda√ß√£o                          |
| ---------------------- | ----------------------- | ------------------------------------- |
| `use_fp16=True`        | -50% VRAM, +velocidade  | ‚úÖ **Sempre usar**                     |
| `use_cuda_kernel=True` | Kernels CUDA compilados | ‚ö†Ô∏è Requer build                        |
| `use_deepspeed=True`   | Acelera autogressivo    | ‚ö†Ô∏è Pode ser mais lento em alguns casos |

### LoRA Fine-Tuning (Vozes Personalizadas)

Para treinar vozes espec√≠ficas de personagens:

**1. Preparar Dataset:**
```bash
# Criar lista: audio_list.txt
# /path/audio1.wav    Transcri√ß√£o do √°udio 1
# /path/audio2.wav    Transcri√ß√£o do √°udio 2

python tools/extract_codec.py \
    --audio_list data/personagem_audio_list.txt \
    --extract_condition \
    --output_dir finetune_data/personagem \
    --model_path checkpoints/gpt.pth \
    --device cuda
```

**2. Configurar Treino (config.yaml):**
```yaml
train:
  epochs: 15
  optimizer:
    learning_rate: 5.0e-5
  lora:
    r: 16              # Rank (8-32)
    lora_alpha: 32     # Scaling
    lora_dropout: 0.1
    target_modules:
      - "attn.c_attn"
      - "attn.c_proj"
      - "mlp.c_fc"
      - "mlp.c_proj"
```

**3. Treinar:**
```bash
python train.py
```

**Requisitos de Dataset:**
- M√≠nimo: ~30 minutos de √°udio
- Ideal: 1-2 horas para m√°xima qualidade
- Transcri√ß√µes precisas obrigat√≥rias

### üìÇ Fontes de Vozes (Assets)
Para clonagem de alta qualidade sem ru√≠do:
1. **Jogos Gacha (Genshin/Star Rail/ZZZ)**:
   - **Ferramenta Recomendada**: [AnimeWwise](https://github.com/Escartem/AnimeWwise). Consegue extrair os √°udios `.pck` recuperando os nomes originais dos arquivos.
   - **Seus Caminhos ZZZ**:
     - **Windows**: `E:\Epic\ZenlessZoneZero\ZenlessZoneZero_Data\StreamingAssets\Audio\Windows\Full`
     - **WSL/Linux**: `/mnt/e/Epic/ZenlessZoneZero/ZenlessZoneZero_Data/StreamingAssets/Audio/Windows/Full`
2. **Visual Novels**: Geralmente arquivos `.wav`/`.ogg` soltos na pasta do jogo.
3. **UVR5 (Ultimate Vocal Remover)**: Usar para limpar vozes de animes com m√∫sica de fundo.

---

## üìÅ ESTRUTURA DO PROJETO

```
IndexTTS2/
‚îú‚îÄ‚îÄ checkpoints/          # Pesos do modelo (~4.7GB total)
‚îÇ   ‚îú‚îÄ‚îÄ gpt.pth          # GPT principal (~3.4GB)
‚îÇ   ‚îú‚îÄ‚îÄ s2mel.pth        # Speech-to-Mel (~1.2GB)
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml      # Configura√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ qwen0.6bemo4-merge/  # Modelo Qwen para emo√ß√µes
‚îú‚îÄ‚îÄ indextts/            # C√≥digo fonte do modelo
‚îÇ   ‚îú‚îÄ‚îÄ infer_v2.py      # IndexTTS2 (atual)
‚îÇ   ‚îú‚îÄ‚îÄ infer.py         # IndexTTS1 (legado)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ examples/            # √Åudios de exemplo
‚îú‚îÄ‚îÄ tools/               # Utilit√°rios
‚îú‚îÄ‚îÄ webui.py             # Interface Gradio
‚îî‚îÄ‚îÄ pyproject.toml       # Depend√™ncias (uv)
```

---

## üêõ PROBLEMAS CONHECIDOS

### Git LFS budget excedido
**Sintoma:** `git lfs pull` falha com "exceeded its LFS budget"  
**Solu√ß√£o:** Baixar exemplos do HuggingFace:
```bash
hf download IndexTeam/IndexTTS-2 --local-dir=checkpoints
```

### CUDA Out of Memory
**Sintoma:** `CUDA out of memory` durante infer√™ncia  
**Solu√ß√£o:**
1. Usar FP16: `use_fp16=True` ou `--fp16`
2. Reduzir batch size
3. Fechar outros processos GPU
4. Usar textos menores

### √Åudio de m√° qualidade
**Causa:** √Åudio de refer√™ncia ruim  
**Solu√ß√£o:**
- Usar √°udio limpo, sem ru√≠do de fundo
- 3-10 segundos de dura√ß√£o ideal
- Sample rate 24kHz+ recomendado

---

## üîó INTEGRA√á√ÉO COM PIPELINE EXISTENTE

O IndexTTS2 pode substituir o XTTS v2 atual em `gpu-node-3090:8020`:

**Endpoint TTS atual (XTTS):**
```bash
curl -X POST "http://100.86.51.120:8020/tts_to_audio/" \
    -H "Content-Type: application/json" \
    -d '{"text": "Ol√° mundo", "language": "pt", "speaker_wav": "default"}' \
    -o output.wav
```

**IndexTTS2 como API:**
- Criar wrapper FastAPI para compatibilidade
- Suporta mesmos par√¢metros + emo√ß√µes
- Melhor qualidade de voz clonada

---

## üìã CHECKLIST PR√â-USO

- [ ] Checkpoints baixados em `checkpoints/`
- [ ] GPU com 8GB+ VRAM livre
- [ ] Ambiente uv sincronizado: `uv sync --all-extras`
- [ ] √Åudios de exemplo dispon√≠veis

---

## üìù MANUTEN√á√ÉO

### Ap√≥s modificar projeto:
```bash
git add .
git commit -m "feat/fix/docs: [descri√ß√£o]"
git push
```

### Atualizar depend√™ncias:
```bash
uv sync --all-extras --upgrade
```

---

## üáßüá∑ REGRA DE IDIOMA

**Todo conte√∫do DEVE ser em Portugu√™s do Brasil (pt-BR).**

- TTS: Suporta `pt`, `en`, `zh` e outros
- C√≥digo: Ingl√™s (vari√°veis, fun√ß√µes)
- Documenta√ß√£o: Portugu√™s BR

---

## üè≠ LORA FACTORY (Multi-Character Voice Generation)

Sistema para criar vozes originais misturando √°udios de jogos (Genshin + ZZZ).

### Arquitetura Remota
```
PC Windows (100.66.250.109)        VM dev-remote-01           GPU gpu-node
E:\AnimeWwise\                     (Desenvolvimento)          (Treinamento)
‚îú‚îÄ Genshin_JP (110+ chars)  ‚îÄ‚îÄSSH‚îÄ‚îÄ‚ñ∫  source_audio/    ‚îÄ‚îÄ‚ñ∫   trained_ckpts/
‚îú‚îÄ Genshin_EN               ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  batch_ingest.py   ‚îÄ‚îÄ‚ñ∫   S3 MinIO
‚îú‚îÄ ZZZ_JP (Ver1.4-2.2)      ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  voice_mixer.py    ‚îÄ‚îÄ‚ñ∫   voice-loras/
‚îî‚îÄ ZZZ_EN                                                   emotion-samples/
```

### Comandos de Sincroniza√ß√£o (dev-remote-01 ‚Üí PC Windows)
```bash
# Testar conex√£o
sshpass -p 'nvidia@amd' ssh user@100.66.250.109 "dir E:\\AnimeWwise"

# Copiar personagem espec√≠fico
sshpass -p 'nvidia@amd' scp -r user@100.66.250.109:"E:\\AnimeWwise\\Genshin_JP\\vo_freetalk\\vo_ayaka\\*" source_audio/vo_ayaka/

# Listar personagens dispon√≠veis
sshpass -p 'nvidia@amd' ssh user@100.66.250.109 "dir E:\\AnimeWwise\\Genshin_JP\\vo_freetalk"
```

### Conven√ß√£o de Nomes
`[game]_[char]_[gender]_[lang]`
- **Games**: `gen` (Genshin), `zzz`, `mix` (h√≠brido)
- **Gender**: `f` (female), `m` (male)
- **Lang**: `jp`, `en`
- **Exemplo**: `mix_ayaka_hutao_f_jp`

### ‚ö†Ô∏è GPU REQUIREMENT (CR√çTICO)
```
TREINAMENTO DEVE SER EXECUTADO NO gpu-node (RTX 4090)
IP: 192.168.31.200 / 100.114.21.15 (Tailscale)
SSH: ssh vmadmin@192.168.31.200

DESENVOLVIMENTO: dev-remote-01 (10.41.80.17)
FONTE DE DADOS: PC Windows (100.66.250.109)
```

### Tokenizer
Usar `checkpoints/bpe.model` (12k vocab, compat√≠vel com `gpt.pth`).

### S3 Cold Storage
```bash
# Upload ap√≥s treino
mc cp trained_ckpts/mix_01_f_jp/model.pth minio/voice-loras/female/mix_01_f_jp/

# Download para produ√ß√£o
mc cp minio/voice-loras/female/mix_01_f_jp/model.pth ./cache/
```

### Voice Mixing (Receita)
```bash
python tools/voice_mixer.py \
    --sources gen_ayaka_f_jp gen_hutao_f_jp \
    --output mix_01_f_jp \
    --ratio 50:50
```

### Emotion Transfer (Dubbing)
```python
tts.infer(
    spk_audio_prompt='ref.wav',
    emo_audio_prompt='emotion-samples/sad/ayaka_sad.wav',  # Sentimento
    emo_alpha=0.7,
    text="Texto da linha...",
    output_path="output.wav"
)
```

### Candidatos Femininos (JP)
| Personagem | Timbre               | Path              |
| ---------- | -------------------- | ----------------- |
| Ayaka      | Elegante, Suave      | `vo_ayaka`        |
| Hu Tao     | Energ√©tica, Aguda    | `vo_hutao`        |
| Raiden     | Autorit√°ria, Grave   | `vo_raidenshogun` |
| Yae Miko   | Sedutora, Misteriosa | `vo_yaemiko`      |
| Ganyu      | Doce, Calma          | `vo_ganyu`        |

---

## üìù HIST√ìRICO

| Data       | Mudan√ßa                                                         |
| ---------- | --------------------------------------------------------------- |
| 2026-01-07 | LoRA Factory architecture documented                            |
| 2026-01-05 | Projeto clonado e configurado                                   |
| 2026-01-08 | Arquitetura remota documentada (dev-remote-01 + PC Windows SSH) |
